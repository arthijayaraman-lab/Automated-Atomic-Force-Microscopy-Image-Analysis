{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s9gj8w-AonXo",
    "outputId": "1e8181c1-9267-4c6c-ab73-071276f1427f"
   },
   "outputs": [],
   "source": [
    "!pip3 install porespy\n",
    "!pip3 install pypardiso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnTyLRaEnK3g"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "import pywt\n",
    "import porespy as ps\n",
    "import scipy.ndimage as spim\n",
    "from skimage.transform import radon\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.fftpack import dct\n",
    "import time\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from matplotlib.ticker import FormatStrFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1Na0ugXrNnw",
    "outputId": "69420705-9a97-4ab4-99dc-7e692a9ef8e7"
   },
   "outputs": [],
   "source": [
    "class Segment:\n",
    "  def __init__(self, pca_comp = 3, pca_apply=False, win_ratio = 0.03, transform_string = \"dft\", stand_out_features = True, eql_hist = False, use_kmeans = True, use_gmm = False, no_clusters = 2, stats_to_use = \"va\", um_pix=5.21):\n",
    "    self.pca_comp = pca_comp    # No. of Principal components to decompose \n",
    "    self.pca_apply = pca_apply  # flag to apply Principal component analysis on features\n",
    "    self.win_ratio = win_ratio  # win factor (decides the size of the tile)\n",
    "    self.transform_string = transform_string  # list of tranforms applied to extract features\n",
    "    self.stand_out_features = stand_out_features  # flag to normalize features\n",
    "    self.eql_hist = eql_hist  # flag to perform histogram equalization of input image \n",
    "    self.use_kmeans = use_kmeans  # flag to use kmeans for clustering \n",
    "    self.use_gmm = use_gmm  # flag to use gaussian mixture models for clustering \n",
    "    self.no_clusters = no_clusters  # No. of Clusters to form\n",
    "    self.stats_to_use = stats_to_use  # list of satistics to use\n",
    "    self.um_pix=um_pix   # length of one pix in nanometers\n",
    "    self.model_stack = []  \n",
    "    self.layer_names = ['conv1_relu', 'conv2_block3_2_relu', 'conv3_block1_2_relu']  # layers to use from ResNet50 for feature extraction\n",
    "    self.transform_list = self.get_transform_list()\n",
    "\n",
    "  def get_transform_list(self):\n",
    "    transform_list = self.transform_string.split(\"_\")\n",
    "    return transform_list\n",
    "      \n",
    "  ############################################\n",
    "  ############# Loop functions ###############\n",
    "  ############################################\n",
    "\n",
    "  def get_features(self, img1, transform):\n",
    "    \"\"\"\n",
    "    function to extract features from tile.\n",
    "    Input:\n",
    "      img1(ndarray) : tile  \n",
    "      transform(list (str)) : list of transform to perform to extract features  \n",
    "    return:\n",
    "      all_features(ndarray)  : feature vector\n",
    "    \"\"\"\n",
    "    if \"dlresnet50\" in transform:\n",
    "      self.load_ResNet50()\n",
    "      all_features = self.get_ResNet50(img1)\n",
    "    else:\n",
    "      all_features = np.array([])\n",
    "    if \"radon\" in transform :\n",
    "      all_features=np.append(all_features,self.get_radon_features(img1), axis=0)\n",
    "    if \"dft\" in transform :\n",
    "      all_features=np.append(all_features,self.get_dft_features(img1), axis=0)\n",
    "    if \"dct\" in transform :\n",
    "      all_features=np.append(all_features,self.get_dct_features(img1), axis=0)\n",
    "    if \"dwt_biort\" in transform :\n",
    "      all_features=np.append(all_features,self.get_biort_features(img1), axis=0)\n",
    "    if \"dwt_haar\" in transform :\n",
    "      all_features=np.append(all_features,self.get_wavelget_haar_featureset_features(img1), axis=0)\n",
    "    return all_features\n",
    "\n",
    "  def slinding_window(self, img):\n",
    "    \"\"\"\n",
    "    function to make tiles\n",
    "    features:\n",
    "      input(ndarray) : input image \n",
    "    return:\n",
    "      all_trans_features(ndarray) : feature cube flattened\n",
    "      i_ctr(int) : feature cube width\n",
    "      j_ctr(int) : feature cube height\n",
    "    \"\"\"\n",
    "    for transform in self.transform_list:\n",
    "      img_features = []\n",
    "      if \"dlresnet50\" in self.transform_list:\n",
    "        win_siz = 32\n",
    "        img_tiles = []\n",
    "      else:\n",
    "        win_siz = round((img.shape[0]*self.win_ratio + img.shape[1]*self.win_ratio)/2)\n",
    "\n",
    "      i_ctr = 0\n",
    "      for i in range(win_siz//2,img.shape[0]-(win_siz//2),1):\n",
    "        j_ctr = 0\n",
    "        for j in range(win_siz//2,img.shape[1]-(win_siz//2),1):\n",
    "          if \"dlresnet50\" in transform:\n",
    "            img_tiles.append(img[(i-(win_siz//2)):(i+(win_siz//2)), (j-(win_siz//2)):(j+(win_siz//2))])\n",
    "          else:\n",
    "            img_features.append(self.get_features(img[(i-(win_siz//2)):(i+(win_siz//2)), (j-(win_siz//2)):(j+(win_siz//2))],transform))\n",
    "          j_ctr+=1\n",
    "        i_ctr+=1\n",
    "      if \"dlresnet50\" in transform:\n",
    "        img_features = self.get_features(img_tiles, transform)\n",
    "      img_features = np.array(img_features)\n",
    "      if \"all_trans_features\" not in locals():\n",
    "        all_trans_features = img_features.copy()\n",
    "      else:\n",
    "        all_trans_features = np.append(all_trans_features, img_features, axis=1)\n",
    "\n",
    "    return all_trans_features, i_ctr, j_ctr\n",
    "\n",
    "  ############################################\n",
    "  ############## DL ResNet50 #################\n",
    "  ############################################\n",
    "\n",
    "  def load_ResNet50(self):\n",
    "    \"\"\"\n",
    "    function to initalize ResNet50 model \n",
    "    Return:\n",
    "      tf_model_object, tf_model_object, ...\n",
    "    \"\"\"\n",
    "    model = tf.keras.applications.resnet50.ResNet50(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=None,\n",
    "        input_shape=(32,32,3),\n",
    "        pooling=None\n",
    "    )\n",
    "\n",
    "    for lyr in self.layer_names:\n",
    "      self.model_stack.append(Model(inputs=model.input, outputs=model.get_layer(lyr).output))\n",
    "    return self.model_stack\n",
    "\n",
    "  def get_ResNet50(self, img):\n",
    "    \"\"\"\n",
    "    function that returns features generated from ResNet50 model.  \n",
    "    Input:\n",
    "        img(ndarray) : tile \n",
    "    return:\n",
    "        out_features(ndarray) : feature vector \n",
    "\n",
    "    \"\"\"\n",
    "    img = np.array(img)\n",
    "    img = np.repeat(img[:, :, :, np.newaxis], 3, axis=3) # repeat gray scale image into 3 channels like rgb\n",
    "\n",
    "    for img_t in np.split(img, 8, axis=0):  #maxPrimeFactors(img.shape[0])\n",
    "      if len(self.model_stack)>0:\n",
    "        output_1 = self.model_stack[0].predict(img_t)\n",
    "        features = np.max(output_1,axis=(1, 2))\n",
    "        features = np.append(features, np.mean(output_1,axis=(1, 2)), axis =1)\n",
    "        features = np.append(features, np.var(output_1,axis=(1, 2)), axis =1)\n",
    "      if len(self.model_stack)>1:\n",
    "        output_2 = self.model_stack[1].predict(img_t)\n",
    "        features = np.append(features, np.max(output_2,axis=(1, 2)), axis =1)\n",
    "        features = np.append(features, np.mean(output_2,axis=(1, 2)), axis =1)\n",
    "        features = np.append(features, np.var(output_2,axis=(1, 2)), axis =1)\n",
    "      if len(self.model_stack)>2:\n",
    "        output_3 = self.model_stack[2].predict(img_t)\n",
    "        features = np.append(features, np.max(output_3,axis=(1, 2)), axis =1)\n",
    "        features = np.append(features, np.mean(output_3,axis=(1, 2)), axis =1)\n",
    "        features = np.append(features, np.var(output_3,axis=(1, 2)), axis =1)\n",
    "      if 'out_features' not in locals():\n",
    "        out_features = features.copy()\n",
    "      else:\n",
    "        out_features = np.append(out_features,features, axis = 0)\n",
    "    \n",
    "    return out_features\n",
    "\n",
    "\n",
    "\n",
    "  ############################################\n",
    "  ########### Domain Transforms ##############\n",
    "  ############################################\n",
    "\n",
    "  ####### Discrete Cosine transforms ########\n",
    "  def get_dct_features(self, tile):\n",
    "    \"\"\"\n",
    "    Discrete cosine transform (DCT) based feature extraction\n",
    "    Input:\n",
    "        tile(ndarray) : tile \n",
    "    return\n",
    "        self.get_stats_feature_vector(dct_image) (ndarray) : feature vectore \n",
    "    \"\"\"\n",
    "    dct_image = dct(dct(tile.T, norm='ortho').T, norm='ortho')\n",
    "    return self.get_stats_feature_vector(dct_image)\n",
    "\n",
    "  ###### Discrete Fourirer transforms ########\n",
    "\n",
    "  def get_dft_features(self, tile):\n",
    "    \"\"\"\n",
    "    Discrete fourier transform (DFT) based feature extraction\n",
    "    Input:\n",
    "        tile(ndarray) : tile \n",
    "    return\n",
    "        self.get_stats_feature_vector(dft_image) (ndarray) : feature vectore \n",
    "    \"\"\"\n",
    "    dft_image = np.fft.fft2(tile)\n",
    "    dft_image = np.fft.fftshift(dft_image)\n",
    "    return self.get_stats_feature_vector(dft_image)\n",
    "\n",
    "\n",
    "  ########## Wavelet transforms ##############\n",
    "  def get_biort_features(self, tile, level=2):\n",
    "    \"\"\"\n",
    "    Discrete wavelet transform (DWT) bi-orthogonal wavelet based feature extraction\n",
    "    Input:\n",
    "        tile(ndarray) : tile \n",
    "        level(int) : level of decomposition\n",
    "    return\n",
    "        texture_features(ndarray) : feature vectore \n",
    "    \"\"\"\n",
    "\n",
    "    coeffs = pywt.swt2(tile, 'bior5.5', start_level=0, level = level)\n",
    "    cD = coeffs[:][1]\n",
    "    texture_features = []\n",
    "    for detail_coeff_ind in range(len(cD)):\n",
    "        for i in range(3):\n",
    "            texture_features.append(self.get_stats_feature_vector(cD[detail_coeff_ind][1][i]))\n",
    "    return texture_features\n",
    "\n",
    "  def get_haar_features(self, tile, level = 4):\n",
    "    \"\"\"\n",
    "    Discrete wavelet transform (DWT) Haar wavelet based feature extraction\n",
    "    Input:\n",
    "        tile(ndarray) : tile \n",
    "        level(int) : level of decomposition\n",
    "    return\n",
    "        texture_features(ndarray) : feature vectore \n",
    "    \"\"\"\n",
    "      \n",
    "    coeffs = pywt.wavedec2(tile, \"haar\", level=level)\n",
    "    cA, cD = coeffs[0], coeffs[1:]\n",
    "    texture_features = []\n",
    "\n",
    "    for detail_coeff in cD:\n",
    "        texture_features.append(self.get_stats_feature_vector(detail_coeff))\n",
    "\n",
    "    return texture_features\n",
    "\n",
    "  ########### Radon transform  ###############\n",
    "  def get_radon_features(self, tile):\n",
    "    \"\"\"\n",
    "    Radon transform based feature extraction\n",
    "    Input:\n",
    "        tile(ndarray) : tile \n",
    "    return\n",
    "        texture_features(ndarray) : feature vectore \n",
    "    \"\"\"\n",
    "    theta = np.linspace(0., 180., max(tile.shape), endpoint=False)\n",
    "    sinogram = radon(tile, theta=theta, circle=False)\n",
    "    texture_features = self.get_stats_feature_vector(sinogram)\n",
    "    return texture_features\n",
    "\n",
    "\n",
    "  ############################################\n",
    "  ########## Clustering functions ############\n",
    "  ############################################\n",
    "\n",
    "  def fit_kmeans(self, feature_cube_flt, w, h):\n",
    "    \"\"\"\n",
    "    predict clusters from feature cube using k-means\n",
    "\n",
    "    Input:\n",
    "        feature_cube_flt(ndarray) : feature cube flattened  \n",
    "        w(int) : width of feature cube\n",
    "        h(int) : height of feature cube\n",
    "    return\n",
    "        index_map(ndarray) : index map \n",
    "    \"\"\"\n",
    "      \n",
    "    kmeans = KMeans(n_clusters=self.no_clusters)\n",
    "    kmeans.fit(feature_cube_flt)\n",
    "    ind_out = kmeans.predict(feature_cube_flt)\n",
    "    out_img = np.zeros((w*h), dtype = float)\n",
    "    for i in range(ind_out.shape[0]):\n",
    "        out_img[i]=ind_out[i]\n",
    "    index_map = out_img.reshape(w,h)\n",
    "    return index_map\n",
    "\n",
    "  def fit_gmm(self, feature_cube_flt, w, h):\n",
    "    \"\"\"\n",
    "    predict clusters from feature cube using gmm\n",
    "    Input:\n",
    "        feature_cube_flt(ndarray) : feature cube flattened  \n",
    "        w(int) : width of feature cube\n",
    "        h(int) : height of feature cube\n",
    "    return\n",
    "        index_map(ndarray) : index map \n",
    "    \"\"\"\n",
    "    gmm = GaussianMixture(n_components=self.no_clusters)\n",
    "    gmm.fit(dataset)\n",
    "    ind_out = gmm.predict(dataset)\n",
    "    out_img = np.zeros((w*h), dtype = float)\n",
    "    for i in range(ind_out.shape[0]):\n",
    "        out_img[i]=ind_out[i]\n",
    "    index_map = out_img.reshape(w,h)\n",
    "    return index_map\n",
    "\n",
    "\n",
    "  ############################################\n",
    "  ############# Aux functions ################\n",
    "  ############################################\n",
    "\n",
    "  ########## stats for feature extraction and dimension reduction ##########\n",
    "  def get_stats_feature_vector(self, trans_out):\n",
    "    \"\"\"\n",
    "    function to multiplex statistics to apply on domain transform outputs\n",
    "    Input:\n",
    "        trans_out(ndarray) : domain transform output\n",
    "    Output:\n",
    "        feature_vector(ndarray) : feature vector \n",
    "    \"\"\"\n",
    "    trans_out = np.abs(trans_out)\n",
    "    trans_out = trans_out.flatten()\n",
    "    feature_vector = []\n",
    "\n",
    "    if \"a\" in self.stats_to_use:\n",
    "      mean_trans_out = np.mean(trans_out)\n",
    "      feature_vector.append(mean_trans_out)\n",
    "\n",
    "    if \"m\" in self.stats_to_use:\n",
    "      max_trans_out = np.max(trans_out)\n",
    "      feature_vector.append(max_trans_out)\n",
    "\n",
    "    if \"v\" in self.stats_to_use:\n",
    "      variance_trans_out = np.var(trans_out)\n",
    "      feature_vector.append(variance_trans_out)\n",
    "\n",
    "    if \"s\" in self.stats_to_use:\n",
    "      mean_trans_out = np.mean(trans_out)\n",
    "      variance_trans_out = np.var(trans_out)\n",
    "      skewness_trans_out = np.mean((trans_out - mean_trans_out) ** 3) / (variance_trans_out ** (3/2))\n",
    "      feature_vector.append(skewness_trans_out)\n",
    "\n",
    "    if \"k\" in self.stats_to_use:\n",
    "      mean_trans_out = np.mean(trans_out)\n",
    "      variance_trans_out = np.var(trans_out)\n",
    "      kurtosis_trans_out = np.mean((trans_out - mean_trans_out) ** 4) / (variance_trans_out ** 2)\n",
    "      feature_vector.append(kurtosis_trans_out)\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "  ########## PCA ##########\n",
    "  def processing_apply_pca(self, feature_cube_flt):\n",
    "    \"\"\"\n",
    "    function to apply Principal Component Analysis\n",
    "    Input:\n",
    "        feature_cube_flt(ndarray) : feature cube flattened \n",
    "    output:\n",
    "        principalComponents(ndarray) : principal components\n",
    "    \"\"\"\n",
    "    feature_cube_flt = self.processing_normalization(feature_cube_flt)\n",
    "    pca = PCA(n_components=self.pca_comp)\n",
    "    principalComponents = pca.fit_transform(feature_cube_flt)\n",
    "    return principalComponents\n",
    "\n",
    "  ########## normalising ##########\n",
    "  def processing_normalization(self, feature_cube_flt):\n",
    "    \"\"\"\n",
    "    Discription: function to normalize feature cube\n",
    "    Input:\n",
    "        feature_cube_flt(ndarray) : feature cube flattened \n",
    "    return:\n",
    "        scaled_features(ndarray) : normalized feature cube flattened \n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(feature_cube_flt)\n",
    "    return scaled_features\n",
    "\n",
    "\n",
    "  def get_diameter_distribution(self, img):\n",
    "    \"\"\"\n",
    "    Function to calculate domain size distribution from index map\n",
    "    uses porosity simulations on images \n",
    "    Input:\n",
    "        img(ndarray) : index map\n",
    "    Returns:\n",
    "        fig(matplotlib object) : domain size distribution \n",
    "    #um_pi -> length of one pix in nanometers\n",
    "    \"\"\"\n",
    "    thk = ps.filters.local_thickness(img)\n",
    "    plt.rcParams.update({'font.size': 14})\n",
    "    fig, ax = plt.subplots(1, 2, figsize=[10, 4], constrained_layout=True)\n",
    "\n",
    "    ax[0].tick_params(left = False, right = False , labelleft = False ,\n",
    "                    labelbottom = False, bottom = False)\n",
    "    #thk = thk*(2*um_pix)  # gives diameter therefore its 2 * scale of pix to nanometers\n",
    "    img0 = ax[0].imshow(thk*2*self.um_pix, cmap='viridis')\n",
    "\n",
    "    fig.colorbar(img0, ax=ax[0], orientation='vertical')\n",
    "    #thk = thk//(2*um_pix)\n",
    "    psd = ps.metrics.pore_size_distribution(im=thk*2*self.um_pix,log=False,bins=11)\n",
    "    #*2*um_pix  # gives diameter therefore its 2 * scale of pix to nanometers\n",
    "    ax[1].plot(psd.bin_centers[:-1], -1*np.diff(psd.cdf), color=\"black\")  # the results from the psd.pdf is very flaky. it gives different scales for different bin sizes and values check this out when free\n",
    "    ax[1].tick_params('x', top=True)\n",
    "    ax[1].tick_params('y', right=True)\n",
    "    ax[1].set_ylim([0, 0.3])\n",
    "    ax[1].yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalize segment class with segmenatation function variables \n",
    "segment = Segment(\n",
    "    pca_comp = 3,\n",
    "    pca_apply=False,\n",
    "    win_ratio = 0.03,\n",
    "    transform_string = \"dft_dlresnet50\", #\"dl_resnet50\",\n",
    "    stand_out_features = True,\n",
    "    eql_hist = False,\n",
    "    use_kmeans = True,\n",
    "    use_gmm = False,\n",
    "    no_clusters = 2,\n",
    "    stats_to_use = \"vas\",\n",
    "    um_pix=5.21,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load microscopy images as rgb or gray scale images. \n",
    "inp_img = np.random.randint(low=0, high=254, size=(60,60), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view input microscopy images \n",
    "plt.imshow(inp_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate feature cube from input microscopy image \n",
    "feature_cube_flat, w, h = segment.slinding_window(inp_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cube_flat = np.array(feature_cube_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization on feature cube\n",
    "feature_cube_flat_norm = segment.processing_normalization(feature_cube_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster feature cube based on similarity \n",
    "index_map = segment.fit_kmeans(feature_cube_flat_norm,w,h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_map = index_map*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view predicted index map\n",
    "plt.imshow(index_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot domain size distribution from index map\n",
    "fig = segment.get_diameter_distribution(index_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dEuzQ-exx1u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
